{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy Dataset Feature_Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loaded = pd.read_csv(\"../Part3_Feature_Engineering/energydata_complete_transformed.csv\")\n",
    "df = df_loaded\n",
    "X = df.drop(['Appliances'],axis=1)\n",
    "y = df['Appliances']\n",
    "\n",
    "# Creating Features Dataframe\n",
    "all_feature = X.columns.tolist()\n",
    "feature_df = pd.DataFrame(index = all_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "> [Source](https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/):\n",
    ">The Recursive Feature Elimination (RFE) method is a feature selection approach. It works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    ">This recipe shows the use of RFE to select 3 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import (LinearRegression, Ridge, Lasso)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a base classifier used to evaluate a subset of attributes\n",
    "model_lr = LinearRegression()\n",
    "model_lasso = Lasso()\n",
    "model_rf = RandomForestRegressor(n_estimators=500)\n",
    "\n",
    "# Create the RFE model and select 3 attributes\n",
    "rfe_lr = RFE(model_lr, n_features_to_select = 3)\n",
    "rfe_lr = rfe_lr.fit(X, y)\n",
    "\n",
    "rfe_lasso = RFE(model_lasso, n_features_to_select = 3)\n",
    "rfe_lasso = rfe_lasso.fit(X, y)\n",
    "\n",
    "rfe_rf = RFE(model_rf, n_features_to_select = 3)\n",
    "rfe_rf = rfe_rf.fit(X, y)\n",
    "\n",
    "# Summarize the selection of the attributes\n",
    "feature_df['RFE_LR_Rank'] = rfe_lr.ranking_\n",
    "feature_df['RFE_Lasso_Rank'] = rfe_lasso.ranking_\n",
    "feature_df['RFE_RF_Rank'] = rfe_rf.ranking_\n",
    "\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    ">[Source](https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/):\n",
    ">Methods that use ensembles of decision trees (like Random Forest or Extra Trees) can also compute the relative importance of each attribute. These importance values can be used to inform a feature selection process.\n",
    "\n",
    ">This recipe shows the construction of an Extra Trees ensemble and displays the relative feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Display the relative importance of each attribute\n",
    "feature_df['Feature_Imp_ETC'] = model.feature_importances_\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Random Forest Model gave the best prediction, using it for feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the dataframe as per Random Forest Rank\n",
    "feature_rank_sort_df = feature_df.sort_values('RFE_RF_Rank')\n",
    "feature_rank_sort_df['Feature'] = feature_rank_sort_df.index\n",
    "feature_rank_sort_df\n",
    "\n",
    "# Plot the ranking of the features\n",
    "sns.factorplot(x='RFE_RF_Rank', y='Feature', data = feature_rank_sort_df, kind=\"bar\", size=14, aspect=1.9, palette='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the dataframe as per Feature Importance\n",
    "feature_imp_sort_df = feature_df.sort_values('Feature_Imp_ETC', ascending = False)\n",
    "feature_imp_sort_df['Feature'] = feature_imp_sort_df.index\n",
    "feature_imp_sort_df\n",
    "\n",
    "# Plot the ranking of the features\n",
    "# sns.factorplot(x='RFE_LR_Rank', y='Feature', data = feature_rank_sort_df, kind=\"bar\", size=14, aspect=1.9, palette='coolwarm');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
